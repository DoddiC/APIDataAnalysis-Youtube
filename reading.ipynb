{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05cc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing findspark and initialize Spark session\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696d8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/Users/DoddiC/Downloads/spark-3.4.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0f5e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd7641cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966bc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark #because we imported findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Dummy Testing Code\n",
    "\n",
    "# Create a sample DataFrame\n",
    "# data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 28)]\n",
    "# columns = [\"Name\", \"Age\"]\n",
    "# sample_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the sample DataFrame\n",
    "# sample_df.show()\n",
    "\n",
    "# Perform a simple operation: Select and display the names of individuals older than 28\n",
    "# result_df = sample_df.filter(sample_df[\"Age\"] > 28).select(\"Name\")\n",
    "# result_df.show()\n",
    "\n",
    "# adding gracefullyshutdown = true for graceful shutdown of processes (running) and getORCreate for reusing a Spark session if it's already running, reducing overhead\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\ \n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f5c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7bc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "# Psuedo: \n",
    "# Goal: Read data from Kafka topic\n",
    "# 1. Specify data format\n",
    "# 2. List Kafka broker addresses\n",
    "# 3. Subscribe to the specified Kafka topic\n",
    "# 4. Set a starting offset to the earliest available\n",
    "# 5. loading the Kafka data into a dataframe (might just be .load())\n",
    "df = spark \\\n",
    "    .read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"first_topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ea5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|    key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+-------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|   [30]|[7B 22 69 64 22 3...|first_topic|        0|     0|2023-06-04 15:53:...|            0|\n",
      "|   [31]|[7B 22 69 64 22 3...|first_topic|        0|     1|2023-06-04 15:53:...|            0|\n",
      "|   [32]|[7B 22 69 64 22 3...|first_topic|        0|     2|2023-06-04 15:53:...|            0|\n",
      "|   [33]|[7B 22 69 64 22 3...|first_topic|        0|     3|2023-06-04 15:53:...|            0|\n",
      "|   [34]|[7B 22 69 64 22 3...|first_topic|        0|     4|2023-06-04 15:53:...|            0|\n",
      "|   [35]|[7B 22 69 64 22 3...|first_topic|        0|     5|2023-06-04 15:53:...|            0|\n",
      "|   [36]|[7B 22 69 64 22 3...|first_topic|        0|     6|2023-06-04 15:53:...|            0|\n",
      "|   [37]|[7B 22 69 64 22 3...|first_topic|        0|     7|2023-06-04 15:53:...|            0|\n",
      "|   [38]|[7B 22 69 64 22 3...|first_topic|        0|     8|2023-06-04 15:53:...|            0|\n",
      "|   [39]|[7B 22 69 64 22 3...|first_topic|        0|     9|2023-06-04 15:53:...|            0|\n",
      "|[31 30]|[7B 22 69 64 22 3...|first_topic|        0|    10|2023-06-04 15:53:...|            0|\n",
      "|[31 31]|[7B 22 69 64 22 3...|first_topic|        0|    11|2023-06-04 15:53:...|            0|\n",
      "|[31 32]|[7B 22 69 64 22 3...|first_topic|        0|    12|2023-06-04 15:53:...|            0|\n",
      "|[31 33]|[7B 22 69 64 22 3...|first_topic|        0|    13|2023-06-04 15:53:...|            0|\n",
      "|[31 34]|[7B 22 69 64 22 3...|first_topic|        0|    14|2023-06-04 15:53:...|            0|\n",
      "|[31 35]|[7B 22 69 64 22 3...|first_topic|        0|    15|2023-06-04 15:53:...|            0|\n",
      "|[31 36]|[7B 22 69 64 22 3...|first_topic|        0|    16|2023-06-04 15:53:...|            0|\n",
      "|[31 37]|[7B 22 69 64 22 3...|first_topic|        0|    17|2023-06-04 15:53:...|            0|\n",
      "|[31 38]|[7B 22 69 64 22 3...|first_topic|        0|    18|2023-06-04 15:53:...|            0|\n",
      "|[31 39]|[7B 22 69 64 22 3...|first_topic|        0|    19|2023-06-04 15:53:...|            0|\n",
      "+-------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "\n",
    "# Seeing if i can edit the default limit (20)\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96d53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"json\", from_json(df[\"json\"], jsonSchema)) \\\n",
    "#                 .select(\"json.*\")\n",
    "\n",
    "# Parse value from binay to string\n",
    "json_df = df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# # Apply Schema to JSON value column and expand the value\n",
    "# from pyspark.sql.functions import from_json\n",
    "\n",
    "# json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcd5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonSchema from Pyspark\n",
    "\n",
    "jsonSchema = StructType([StructField(\"id\", StringType(), True), StructField(\"Year\", StringType(), True),\n",
    "                                     StructField(\"Reporting_Airline\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a25fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], jsonSchema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8745f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------------+\n",
      "| id|Year|Reporting_Airline|\n",
      "+---+----+-----------------+\n",
      "|  0|1998|               NW|\n",
      "|  1|2009|               FL|\n",
      "|  2|2013|               MQ|\n",
      "|  3|2010|               DL|\n",
      "|  4|2006|               US|\n",
      "|  5|1995|               DL|\n",
      "|  6|2006|               CO|\n",
      "|  7|2019|               9E|\n",
      "|  8|2008|               YV|\n",
      "|  9|2018|               WN|\n",
      "| 10|1991|               US|\n",
      "| 11|2014|               WN|\n",
      "| 12|1994|               AA|\n",
      "| 13|2013|               OO|\n",
      "| 14|2003|               UA|\n",
      "| 15|1988|               PI|\n",
      "| 16|2007|               NW|\n",
      "| 17|2015|               AS|\n",
      "| 18|2006|               UA|\n",
      "| 19|2017|               WN|\n",
      "+---+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_expanded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af5acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Reporting_Airline|count|\n",
      "+-----------------+-----+\n",
      "|               US|   40|\n",
      "|               YV|    6|\n",
      "|               WN|   72|\n",
      "|               PI|    4|\n",
      "|               EV|   11|\n",
      "|               HP|   11|\n",
      "|               NW|   25|\n",
      "|               MQ|   22|\n",
      "|               DL|   77|\n",
      "|               OO|   27|\n",
      "|               UA|   39|\n",
      "|               XE|    8|\n",
      "|               EA|    1|\n",
      "|               YX|    2|\n",
      "|               TW|    9|\n",
      "|               B6|    7|\n",
      "|               F9|    4|\n",
      "|               PS|    1|\n",
      "|               FL|    5|\n",
      "|               CO|   31|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_expanded_df.select('Reporting_Airline').groupBy('Reporting_Airline').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db3cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = json_expanded_df.select('id','Year','Reporting_Airline').filter('Year >= 2015')\n",
    "\n",
    "json_expanded_df.createOrReplaceTempView(\"json_expanded_table\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT id, Year, Reporting_Airline\n",
    "    FROM json_expanded_table\n",
    "    WHERE Year >= 2015\n",
    "\"\"\"\n",
    "\n",
    "df_filtered = spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
